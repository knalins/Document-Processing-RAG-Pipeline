{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Questions Answering Tool using RAG Engine\n",
        "\n",
        "**Before you begin**, please ensure you've **configured the following requirements within the `main()` function** (typically in `main.py` or a dedicated configuration section) in STEP 1:\n",
        "\n",
        "* **LLMWhispererV2 API Key:** PDF data extraction tool.\n",
        "* **Google GenAI API Key:** Retrieval Augmented Generation (RAG) engine and Embeddings that generates answers.\n",
        "* **`Dataformodel.txt`:** Knowledge base or contextual data for your RAG engine.\n",
        "* **`INPUTPDF`:** PDF document from which questions will be extracted.\n",
        "* **Adobe PDF Services Client ID:** For conversion of the generated `DOCX file` into a `final PDF document`.\n",
        "* **Adobe PDF Services Client Secret:** The corresponding secret key for authentication with Adobe PDF Services.\n",
        "---"
      ],
      "metadata": {
        "id": "zjIVs5ECHY-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to Use this Tool?\n",
        "\n",
        "Getting started with this Colab notebook is straightforward. Just follow these steps:\n",
        "\n",
        "1.  **Organize Your Files:** Ensure all necessary files – `req.txt`, your `INPUTPDF` (the PDF you want to process), `Dataformodel.txt`, and `main.py` – are placed in the **same directory** within your Colab environment or local project folder.\n",
        "\n",
        "2.  **Install Dependencies:** Open your terminal or Colab notebook cell and run the following command to install all required libraries:\n",
        "    ```bash\n",
        "    pip install -r req.txt\n",
        "    ```\n",
        "\n",
        "3.  **Configure API Keys & File Paths:** Before running, you'll need to update the `main()` function within your `main.py` file. **Fill in all placeholder API keys** and **correctly specify the file addresses** for your `INPUTPDF` and `Dataformodel.txt`.\n",
        "\n",
        "4.  **Just Run it:**\n",
        "---"
      ],
      "metadata": {
        "id": "zPGWNA39JNeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How it Works: The Pipeline\n",
        "\n",
        "1.  PDF to String **(`LLMWhispererV2`):** Your `INPUT PDF` is transformed into a clean, **`string`**.\n",
        "2.  Question Extraction: An `extract_all_questions` cleans the text and extracts **Numbered Questions**, **Table Questions**, and **Multiple Choice Questions (MCQs)** in dictionary.\n",
        "3.  **RAG Engine (Google GenAI):** It takes `String` for Question & `Dataformodel.txt` for reference. It generates `String with Q & A`.\n",
        "4. RAG String to `DOCX`: `convert_string_to_docx` function converts RAG Engine output string to `OutPut2025##.docx` file.\n",
        "4.  DOCX & PDF Generation **(Adobe PDF Services API):** `docxToPdfConverter` class converts `.docx` file to `output_Output2025##.pdf` file.\n",
        "---"
      ],
      "metadata": {
        "id": "-bEvVi3aKmcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r req.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J00PFe_SGNvB",
        "outputId": "675469ed-ef70-450d-dde2-d6effd4c9db9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Document Processing Pipeline with LLMWhisperer, RAG Engine, and PDF Conversion\n",
        "================================================================================\n",
        "This script processes PDF documents, extracts questions, generates answers using RAG,\n",
        "and converts the results back to PDF format.\n",
        "\"\"\"\n",
        "\n",
        "# ========================================\n",
        "# IMPORTS\n",
        "# ========================================\n",
        "\n",
        "# Data Extraction by LLMWhisperer\n",
        "from unstract.llmwhisperer import LLMWhispererClientV2\n",
        "from unstract.llmwhisperer.client_v2 import LLMWhispererClientException\n",
        "import time\n",
        "\n",
        "# RAG Engine\n",
        "import google.generativeai as genai\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from IPython.display import Markdown, display\n",
        "from llama_index.core import ServiceContext  # Delete it\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "from google.genai.types import EmbedContentConfig\n",
        "\n",
        "# Utilities\n",
        "import os\n",
        "import re\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Adobe PDF Services\n",
        "from adobe.pdfservices.operation.auth.service_principal_credentials import ServicePrincipalCredentials\n",
        "from adobe.pdfservices.operation.exception.exceptions import ServiceApiException, ServiceUsageException, SdkException\n",
        "from adobe.pdfservices.operation.io.cloud_asset import CloudAsset\n",
        "from adobe.pdfservices.operation.io.stream_asset import StreamAsset\n",
        "from adobe.pdfservices.operation.pdf_services import PDFServices\n",
        "from adobe.pdfservices.operation.pdf_services_media_type import PDFServicesMediaType\n",
        "from adobe.pdfservices.operation.pdfjobs.jobs.create_pdf_job import CreatePDFJob\n",
        "from adobe.pdfservices.operation.pdfjobs.result.create_pdf_result import CreatePDFResult\n",
        "\n",
        "# String to Docx\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# =============================================================================\n",
        "# CORE FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def API_Setup_Files_Load(WhispererAPIKEY, GoogleAPIKEY, Localdatabase_Address, Input_File_Path):\n",
        "    \"\"\"\n",
        "    Setup APIs and process document extraction\n",
        "\n",
        "    Args:\n",
        "        WhispererAPIKEY (str): API key for LLMWhisperer\n",
        "        GoogleAPIKEY (str): API key for Google GenAI\n",
        "        Localdatabase_Address (str): Path to local database file\n",
        "        Input_File_Path (str): Path to input file for processing\n",
        "\n",
        "    Returns:\n",
        "        tuple: (extraction_result, vector_index)\n",
        "    \"\"\"\n",
        "    # LLMWhisperer Setup\n",
        "    client = LLMWhispererClientV2(\n",
        "        base_url=\"https://llmwhisperer-api.us-central.unstract.com/api/v2\",\n",
        "        api_key=WhispererAPIKEY\n",
        "    )\n",
        "\n",
        "    # Google GenAI Gemini API Setup\n",
        "    genai.configure(api_key=GoogleAPIKEY)\n",
        "    Settings.llm = GoogleGenAI(models='gemini-1.5-flash-latest', api_key=GoogleAPIKEY)  # Use gemini-2.0-flash\n",
        "    Settings.embed_model = GoogleGenAIEmbedding(\n",
        "        model_name=\"models/embedding-001\",\n",
        "        api_key=GoogleAPIKEY\n",
        "    )\n",
        "    Settings.node_parser = SentenceSplitter(chunk_size=800, chunk_overlap=20)\n",
        "    # Local database for references & retrieval\n",
        "    documents = SimpleDirectoryReader(input_files=[Localdatabase_Address])\n",
        "    doc = documents.load_data()\n",
        "    index = VectorStoreIndex.from_documents(doc)\n",
        "\n",
        "    # LLM Whisperer Working Data Extraction\n",
        "    try:\n",
        "        result = client.whisper(file_path=Input_File_Path)\n",
        "\n",
        "        if result[\"status_code\"] == 202:\n",
        "            print(\"Whisper request accepted.\")\n",
        "            print(f\"Whisper hash: {result['whisper_hash']}\")\n",
        "\n",
        "            while True:\n",
        "                print(\"Polling for whisper status...\")\n",
        "                status = client.whisper_status(whisper_hash=result[\"whisper_hash\"])\n",
        "\n",
        "                if status[\"status\"] == \"processing\":\n",
        "                    print(\"STATUS: processing...\")\n",
        "                elif status[\"status\"] == \"delivered\":\n",
        "                    print(\"STATUS: Already delivered!\")\n",
        "                    break\n",
        "                elif status[\"status\"] == \"unknown\":\n",
        "                    print(\"STATUS: unknown...\")\n",
        "                    break\n",
        "                elif status[\"status\"] == \"processed\":\n",
        "                    print(\"STATUS: processed!\")\n",
        "                    print(\"Let's retrieve the result of the extraction...\")\n",
        "                    resulty = client.whisper_retrieve(whisper_hash=result[\"whisper_hash\"])\n",
        "                    print(resulty)\n",
        "                    return resulty, index\n",
        "\n",
        "                # Poll every 5 seconds\n",
        "                time.sleep(5)\n",
        "\n",
        "    except LLMWhispererClientException as e:\n",
        "        print(e)\n",
        "\n",
        "\n",
        "def extract_all_questions(document_text):\n",
        "    \"\"\"\n",
        "    Extract different types of questions from document text\n",
        "\n",
        "    Args:\n",
        "        document_text (str): Raw extracted text from document\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing numbered_questions, table_questions, and mcq_raw_sections\n",
        "    \"\"\"\n",
        "    numbered_questions = []\n",
        "    table_questions = []\n",
        "    mcq_raw_sections = []\n",
        "\n",
        "    # Normalize newlines and handle form feed character\n",
        "    document_text = re.sub(r'\\r\\n', '\\n', document_text)\n",
        "    # Replace the form feed character with a consistent marker for splitting\n",
        "    document_text = document_text.replace('<<<', '<<<\\n---FORM_FEED---\\n')\n",
        "    # Reduce multiple blank lines to single blank lines\n",
        "    document_text = re.sub(r'\\n\\s*\\n', '\\n\\n', document_text).strip()\n",
        "\n",
        "    # --- 1. Extract numbered questions ---\n",
        "    numbered_questions_section_match = re.search(\n",
        "        r'Questions for Test\\s*\\n(.*?)(?=\\n\\s*Question\\s+Answer|\\Z)',\n",
        "        document_text,\n",
        "        re.DOTALL\n",
        "    )\n",
        "\n",
        "    if numbered_questions_section_match:\n",
        "        numbered_questions_text = numbered_questions_section_match.group(1).strip()\n",
        "        questions_raw = re.split(r'^\\s*\\d+\\.\\s*', numbered_questions_text, flags=re.MULTILINE)\n",
        "\n",
        "        for q_raw in questions_raw:\n",
        "            cleaned_q = q_raw.strip()\n",
        "            if cleaned_q:\n",
        "                # Replace internal newlines and multiple spaces with a single space\n",
        "                cleaned_q = re.sub(r'\\s*\\n\\s*', ' ', cleaned_q)\n",
        "                numbered_questions.append(cleaned_q)\n",
        "\n",
        "    # --- 2. Extract table questions ---\n",
        "    table_section_match = re.search(\n",
        "        r'Question\\s+Answer\\s*\\n(.*?)(?=\\n<<<|\\n\\s*Tick all that applies:|\\Z)',\n",
        "        document_text,\n",
        "        re.DOTALL\n",
        "    )\n",
        "\n",
        "    if table_section_match:\n",
        "        table_content = table_section_match.group(1).strip()\n",
        "        current_question = []\n",
        "\n",
        "        for line in table_content.split('\\n'):\n",
        "            stripped_line = line.strip()\n",
        "            if not stripped_line:\n",
        "                continue  # Skip empty lines\n",
        "\n",
        "            # If the line starts with a capital letter or a commonly used question word, it's likely a new question.\n",
        "            # Otherwise, it's a continuation of the previous question.\n",
        "            if re.match(r'^[A-Z][a-zA-Z]*', stripped_line) and not current_question:\n",
        "                # First line of a new question\n",
        "                current_question.append(stripped_line)\n",
        "            elif re.match(r'^[A-Z][a-zA-Z]*', stripped_line) and current_question and len(current_question[0].split()) > 1:\n",
        "                # New question, but the previous one was complete\n",
        "                table_questions.append(\" \".join(current_question))\n",
        "                current_question = [stripped_line]\n",
        "            elif current_question:\n",
        "                # Continuation of the current question\n",
        "                current_question.append(stripped_line)\n",
        "            else:\n",
        "                # Handle cases where the first line doesn't fit the 'new question' pattern perfectly\n",
        "                current_question.append(stripped_line)\n",
        "\n",
        "        # Add the last accumulated question\n",
        "        if current_question:\n",
        "            table_questions.append(\" \".join(current_question))\n",
        "\n",
        "    # --- 3. Extract raw MCQ section ---\n",
        "    # This section is specifically between '<<<\\n---FORM_FEED---\\n' and the next '<<<\\n---FORM_FEED---\\n' or end of document\n",
        "    mcq_section_match = re.search(\n",
        "        r'Tick all that applies:.*?(?=\\n---FORM_FEED---|\\Z)',\n",
        "        document_text,\n",
        "        re.DOTALL\n",
        "    )\n",
        "\n",
        "    if mcq_section_match:\n",
        "        mcq_raw_sections.append(mcq_section_match.group(0).strip())\n",
        "\n",
        "    return {\n",
        "        'numbered_questions': numbered_questions,\n",
        "        'table_questions': table_questions,\n",
        "        'mcq_raw_sections': mcq_raw_sections\n",
        "    }\n",
        "\n",
        "\n",
        "def clean_xml_incompatible_chars(text):\n",
        "    \"\"\"\n",
        "    Removes characters that are not compatible with XML.\n",
        "    This includes NULL bytes and other control characters (except common ones like tab, newline, carriage return).\n",
        "    \"\"\"\n",
        "    control_chars_regex = r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]'\n",
        "    cleaned_text = re.sub(control_chars_regex, '', text)\n",
        "    return cleaned_text\n",
        "\n",
        "def convert_string_to_docx(input_string, filename=\"output.docx\", font_name=\"Calibri\", font_size=12):\n",
        "    \"\"\"\n",
        "    Converts a given string into a DOCX file, cleaning incompatible XML characters first.\n",
        "\n",
        "    Args:\n",
        "        input_string (str): The string content to be written to the DOCX file.\n",
        "        filename (str): The desired name for the output DOCX file.\n",
        "        font_name (str): Optional. The name of the font to use.\n",
        "        font_size (int): Optional. The font size in points.\n",
        "    \"\"\"\n",
        "    # Clean the input string before processing\n",
        "    cleaned_input_string = clean_xml_incompatible_chars(input_string)\n",
        "\n",
        "    document = Document()\n",
        "\n",
        "    # Add a paragraph and run to insert the string content\n",
        "    paragraph = document.add_paragraph()\n",
        "    run = paragraph.add_run(cleaned_input_string) # Use the cleaned string\n",
        "\n",
        "    # Apply font and size\n",
        "    font = run.font\n",
        "    font.name = font_name\n",
        "    font.size = Pt(font_size)\n",
        "\n",
        "    try:\n",
        "        document.save(filename)\n",
        "        print(f\"Successfully converted string to '{filename}' with font '{font_name}' and size {font_size}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while saving the document: {e}\")\n",
        "\n",
        "\n",
        "class docxToPdfConverter:\n",
        "    \"\"\"\n",
        "    Convert text files to PDF using Adobe PDF Services\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_txt_path, output_pdf_path, client_id, client_secret):\n",
        "        self.input_txt_path = input_txt_path\n",
        "        self.output_pdf_path = output_pdf_path\n",
        "        self.client_id = client_id\n",
        "        self.client_secret = client_secret\n",
        "        self.process_conversion()\n",
        "\n",
        "    def process_conversion(self):\n",
        "        \"\"\"Process the text to PDF conversion\"\"\"\n",
        "        try:\n",
        "            # 1. Read the input docx file as a byte stream\n",
        "            with open(self.input_txt_path, 'rb') as file:\n",
        "                input_stream = file.read()\n",
        "\n",
        "            # 2. Initial setup: create credentials instance using ServicePrincipalCredentials\n",
        "            credentials = ServicePrincipalCredentials(\n",
        "                client_id=self.client_id,\n",
        "                client_secret=self.client_secret\n",
        "            )\n",
        "\n",
        "            # 3. Creates a PDF Services instance\n",
        "            pdf_services = PDFServices(credentials=credentials)\n",
        "\n",
        "            # 4. Creates an asset from the source file and uploads it to Adobe PDF Services\n",
        "            input_asset = pdf_services.upload(input_stream=input_stream, mime_type=PDFServicesMediaType.DOCX)\n",
        "\n",
        "            # 5. Creates a new CreatePDFJob instance\n",
        "            create_pdf_job = CreatePDFJob(input_asset=input_asset)\n",
        "\n",
        "            # 6. Submit the job and gets the job result\n",
        "            logging.info(f\"Attempting to convert '{self.input_txt_path}' to PDF...\")\n",
        "            location = pdf_services.submit(create_pdf_job)\n",
        "            pdf_services_response = pdf_services.get_job_result(location, CreatePDFResult)\n",
        "\n",
        "            # 7. Get content from the resulting asset\n",
        "            result_asset: CloudAsset = pdf_services_response.get_result().get_asset()\n",
        "            stream_asset: StreamAsset = pdf_services.get_content(result_asset)\n",
        "\n",
        "            # 8. Create the output directory if it doesn't exist\n",
        "            os.makedirs(os.path.dirname(self.output_pdf_path), exist_ok=True)\n",
        "\n",
        "            # 9. Creates an output stream and copy stream asset's content to it\n",
        "            with open(self.output_pdf_path, \"wb\") as file:\n",
        "                file.write(stream_asset.get_input_stream())\n",
        "\n",
        "            logging.info(f\"Successfully converted '{self.input_txt_path}' to '{self.output_pdf_path}'\")\n",
        "\n",
        "        except (ServiceApiException, ServiceUsageException, SdkException) as e:\n",
        "            logging.exception(f'Exception encountered while executing operation: {e}')\n",
        "        except Exception as e:\n",
        "            logging.exception(f'An unexpected error occurred: {e}')\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN EXECUTION\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    # Configure logging for better visibility into SDK operations\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    # =============================================================================\n",
        "    # STEP 1: API SETUP AND DATA EXTRACTION (Starter)\n",
        "    # =============================================================================\n",
        "\n",
        "    resulty, index = API_Setup_Files_Load(\n",
        "        WhispererAPIKEY=\"WhispererLLM API Key\",#Put Your WhispererLLM API Key Here\n",
        "        GoogleAPIKEY=\"Google GenAI API Key\", #Put Your Google GenAI API Key Here\n",
        "        Localdatabase_Address=\"Dataformodel.txt\", #Put Your Local Database Address for RAG Engine Here\n",
        "        Input_File_Path=\"Questions_for_Test.pdf\" #Put Your Input File Path Here\n",
        "    )\n",
        "\n",
        "    # Configuration\n",
        "    CLIENT_ID = \"PDF Services Client ID\" #Put Your Adobe PDF Services Client ID from Adobe Developer Console Here\n",
        "    CLIENT_SECRET = \"PDF Services Client Secret\" #Put Your Adobe PDF Services Client Secret from Adobe Developer Console Here\n",
        "    # =============================================================================\n",
        "    # STEP 2: EXTRACT AND ORGANIZE DATA\n",
        "    # =============================================================================\n",
        "\n",
        "    # Storing PDF Data in String format in location Variable(extracted_data)\n",
        "    extracted_data = resulty['extraction']['result_text']\n",
        "    print(type(extracted_data))  # String\n",
        "    print(extracted_data)\n",
        "\n",
        "    # Extraction of Questions for LLMs from Raw Text\n",
        "    Organised_Extracted_data = extract_all_questions(extracted_data)\n",
        "    print(Organised_Extracted_data)\n",
        "\n",
        "    # =============================================================================\n",
        "    # STEP 3: PREPARE QUERIES FOR OUTPUT ENGINE AND GENERATE RESPONSES\n",
        "    # =============================================================================\n",
        "    index.storage_context.persist()\n",
        "    query_engine = index.as_query_engine()\n",
        "    response_from_engine = \"\"\n",
        "    for(key, value) in Organised_Extracted_data.items():\n",
        "        data=Organised_Extracted_data[key]\n",
        "        if(key==\"numbered_questions\"):\n",
        "            engine_query = \"Give both question and Answer\\n\" + '\\n'.join(data)\n",
        "        elif(key==\"table_questions\"):\n",
        "            engine_query = \"Keep both Question with answers in one line\\n\" + '\\n'.join(data)\n",
        "        elif(key==\"mcq_raw_sections\"):\n",
        "            engine_query = \"Keep same text with marked answers\\n\" + '\\n'.join(data)\n",
        "        response_from_engine = response_from_engine+query_engine.query(engine_query).response+\"\\n\"\n",
        "\n",
        "\n",
        "    # =============================================================================\n",
        "    # STEP 5: SAVE RESULTS TO DOCX FILE\n",
        "    # =============================================================================\n",
        "    Local_Output_Path=f\"Output{datetime.now().strftime('%Y-%m-%dT%H-%M-%S')}.docx\"\n",
        "\n",
        "    convert_string_to_docx(\n",
        "        response_from_engine, # Use your 'extracted_data' variable here\n",
        "        filename=Local_Output_Path, ## Output Filename\n",
        "        font_name=\"Arial\",\n",
        "        font_size=14\n",
        "        )\n",
        "\n",
        "    # f = open(Local_Output_Path, \"w\")\n",
        "    # f.write(response_from_engine)\n",
        "    # f.close()\n",
        "    # =============================================================================\n",
        "    # STEP 6: CONVERT DOCX TO PDF\n",
        "    # =============================================================================\n",
        "\n",
        "    # Define your input and output file paths\n",
        "\n",
        "    OUTPUT_PDF_DIRECTORY = \"output/TxtToPdfConversion\"  # Directory for output\n",
        "    OUTPUT_PDF_FILENAME = f\"output_{Local_Output_Path}.pdf\"\n",
        "    INPUT_docx_FILE = Local_Output_Path ##File For pdf creation\n",
        "    OUTPUT_PDF_PATH = os.path.join(OUTPUT_PDF_DIRECTORY, OUTPUT_PDF_FILENAME)\n",
        "    # Run the conversion\n",
        "    converter = docxToPdfConverter(INPUT_docx_FILE, OUTPUT_PDF_PATH, CLIENT_ID, CLIENT_SECRET)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# EXECUTION\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AECVgx8CDQMS",
        "outputId": "a68a345b-c3f5-44ec-b41e-f12c9a4326c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-30 21:31:56,279 - unstract.llmwhisperer.client_v2 - DEBUG - logging_level set to DEBUG\n",
            "DEBUG:unstract.llmwhisperer.client_v2:logging_level set to DEBUG\n",
            "2025-05-30 21:31:56,289 - unstract.llmwhisperer.client_v2 - DEBUG - base_url set to https://llmwhisperer-api.us-central.unstract.com/api/v2\n",
            "DEBUG:unstract.llmwhisperer.client_v2:base_url set to https://llmwhisperer-api.us-central.unstract.com/api/v2\n",
            "2025-05-30 21:32:11,246 - unstract.llmwhisperer.client_v2 - DEBUG - whisper called\n",
            "DEBUG:unstract.llmwhisperer.client_v2:whisper called\n",
            "2025-05-30 21:32:11,248 - unstract.llmwhisperer.client_v2 - DEBUG - api_url: https://llmwhisperer-api.us-central.unstract.com/api/v2/whisper\n",
            "DEBUG:unstract.llmwhisperer.client_v2:api_url: https://llmwhisperer-api.us-central.unstract.com/api/v2/whisper\n",
            "2025-05-30 21:32:11,250 - unstract.llmwhisperer.client_v2 - DEBUG - params: {'mode': 'form', 'output_mode': 'layout_preserving', 'page_seperator': '<<<', 'pages_to_extract': '', 'median_filter_size': 0, 'gaussian_blur_radius': 0, 'line_splitter_tolerance': 0.4, 'horizontal_stretch_factor': 1.0, 'mark_vertical_lines': False, 'mark_horizontal_lines': False, 'line_spitter_strategy': 'left-priority', 'add_line_nos': False, 'lang': 'eng', 'tag': 'default', 'filename': '', 'webhook_metadata': '', 'use_webhook': ''}\n",
            "DEBUG:unstract.llmwhisperer.client_v2:params: {'mode': 'form', 'output_mode': 'layout_preserving', 'page_seperator': '<<<', 'pages_to_extract': '', 'median_filter_size': 0, 'gaussian_blur_radius': 0, 'line_splitter_tolerance': 0.4, 'horizontal_stretch_factor': 1.0, 'mark_vertical_lines': False, 'mark_horizontal_lines': False, 'line_spitter_strategy': 'left-priority', 'add_line_nos': False, 'lang': 'eng', 'tag': 'default', 'filename': '', 'webhook_metadata': '', 'use_webhook': ''}\n",
            "2025-05-30 21:32:11,470 - unstract.llmwhisperer.client_v2 - DEBUG - whisper_status called\n",
            "DEBUG:unstract.llmwhisperer.client_v2:whisper_status called\n",
            "2025-05-30 21:32:11,472 - unstract.llmwhisperer.client_v2 - DEBUG - url: https://llmwhisperer-api.us-central.unstract.com/api/v2/whisper-status\n",
            "DEBUG:unstract.llmwhisperer.client_v2:url: https://llmwhisperer-api.us-central.unstract.com/api/v2/whisper-status\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whisper request accepted.\n",
            "Whisper hash: 255f3dc9|a5f9d55aef9b2f47bccb31a821d17ba8\n",
            "Polling for whisper status...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-30 21:32:16,556 - unstract.llmwhisperer.client_v2 - DEBUG - whisper_status called\n",
            "DEBUG:unstract.llmwhisperer.client_v2:whisper_status called\n",
            "2025-05-30 21:32:16,558 - unstract.llmwhisperer.client_v2 - DEBUG - url: https://llmwhisperer-api.us-central.unstract.com/api/v2/whisper-status\n",
            "DEBUG:unstract.llmwhisperer.client_v2:url: https://llmwhisperer-api.us-central.unstract.com/api/v2/whisper-status\n",
            "2025-05-30 21:32:16,671 - unstract.llmwhisperer.client_v2 - DEBUG - whisper_retrieve called\n",
            "DEBUG:unstract.llmwhisperer.client_v2:whisper_retrieve called\n",
            "2025-05-30 21:32:16,673 - unstract.llmwhisperer.client_v2 - DEBUG - url: https://llmwhisperer-api.us-central.unstract.com/api/v2/whisper-retrieve\n",
            "DEBUG:unstract.llmwhisperer.client_v2:url: https://llmwhisperer-api.us-central.unstract.com/api/v2/whisper-retrieve\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polling for whisper status...\n",
            "STATUS: processed!\n",
            "Let's retrieve the result of the extraction...\n",
            "{'status_code': 200, 'extraction': {'confidence_metadata': [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [{'confidence': '0.876', 'offset': 429, 'text': '[', 'width': 31}], [{'confidence': '0.592', 'offset': 491, 'text': '1', 'width': 20}], [], [{'confidence': '0.518', 'offset': 490, 'text': '1', 'width': 17}, {'confidence': '0.848', 'offset': 431, 'text': '[', 'width': 19}], [{'confidence': '0.531', 'offset': 489, 'text': ']', 'width': 16}], [], [{'confidence': '0.579', 'offset': 425, 'text': '[', 'width': 27}], [{'confidence': '0.589', 'offset': 489, 'text': '1', 'width': 18}, {'confidence': '0.519', 'offset': 491, 'text': '1', 'width': 20}], [{'confidence': '0.626', 'offset': 490, 'text': '1', 'width': 20}], [{'confidence': '0.772', 'offset': 428, 'text': '[', 'width': 26}], [{'confidence': '0.552', 'offset': 489, 'text': ']', 'width': 21}], [], [{'confidence': '0.56', 'offset': 426, 'text': '[', 'width': 30}], [{'confidence': '0.574', 'offset': 490, 'text': '1', 'width': 21}, {'confidence': '0.586', 'offset': 489, 'text': '1', 'width': 17}], [{'confidence': '0.538', 'offset': 491, 'text': ']', 'width': 20}], [], [], [], [{'confidence': '0.584', 'offset': 427, 'text': '[', 'width': 28}], [{'confidence': '0.521', 'offset': 491, 'text': '1', 'width': 19}, {'confidence': '0.837', 'offset': 431, 'text': '[', 'width': 24}], [{'confidence': '0.52', 'offset': 491, 'text': '1', 'width': 17}, {'confidence': '0.788', 'offset': 431, 'text': '[', 'width': 23}], [{'confidence': '0.519', 'offset': 491, 'text': '1', 'width': 20}, {'confidence': '0.783', 'offset': 429, 'text': '[', 'width': 27}], [{'confidence': '0.861', 'offset': 491, 'text': '1', 'width': 16}], [], [], [{'confidence': '0.792', 'offset': 429, 'text': '[', 'width': 26}], [{'confidence': '0.519', 'offset': 491, 'text': '1', 'width': 17}], [], [{'confidence': '0.521', 'offset': 491, 'text': '1', 'width': 18}], [], []], 'line_metadata': [[0, 0, 0, 3168], [0, 0, 0, 0], [0, 359, 65, 3168], [0, 433, 60, 3168], [0, 0, 0, 0], [0, 732, 64, 3168], [0, 811, 62, 3168], [0, 0, 0, 0], [0, 1099, 62, 3168], [0, 1180, 62, 3168], [0, 0, 0, 0], [0, 1470, 64, 3168], [0, 0, 0, 0], [0, 1774, 65, 3168], [0, 0, 0, 0], [0, 2139, 49, 3168], [0, 0, 0, 0], [0, 2247, 59, 3168], [0, 0, 0, 0], [0, 2349, 51, 3168], [0, 0, 0, 0], [0, 2457, 56, 3168], [0, 0, 0, 0], [0, 2573, 56, 3168], [0, 0, 0, 0], [0, 2677, 57, 3168], [0, 2730, 46, 3168], [0, 0, 0, 0], [0, 0, 0, 0], [1, 353, 61, 3168], [1, 422, 55, 3168], [1, 500, 57, 3168], [1, 573, 54, 3168], [1, 649, 55, 3168], [1, 721, 54, 3168], [1, 0, 0, 0], [1, 1016, 65, 3168], [1, 1094, 60, 3168], [1, 1166, 60, 3168], [1, 1239, 54, 3168], [1, 1314, 57, 3168], [1, 0, 0, 0], [1, 1461, 57, 3168], [1, 1536, 56, 3168], [1, 1611, 53, 3168], [1, 1684, 58, 3168], [1, 1758, 55, 3168], [1, 0, 0, 0], [1, 1905, 55, 3168], [1, 1982, 55, 3168], [1, 2055, 60, 3168], [1, 2127, 57, 3168], [1, 2204, 56, 3168], [1, 0, 0, 0], [1, 2349, 55, 3168], [1, 2426, 57, 3168], [1, 2499, 59, 3168], [1, 2570, 53, 3168], [1, 2648, 58, 3168], [1, 0, 0, 0], [1, 0, 0, 0]], 'metadata': {'0': {'font_info': {'average_char_height': 58.883714040630124, 'average_char_width': 27.398887514029933, 'std_dev_char_height': 4.244703630198783, 'std_dev_char_width': 3.540954799235304}, 'line_count': 28, 'line_end': 29, 'line_start': 1, 'page_number': 0, 'potential_subtitles': []}, '1': {'font_info': {'average_char_height': 54.36696167668516, 'average_char_width': 25.95826858827901, 'std_dev_char_height': 3.2060476084751195, 'std_dev_char_width': 4.700210557771893}, 'line_count': 33, 'line_end': 63, 'line_start': 30, 'page_number': 1, 'potential_subtitles': [{'page': 1, 'position': 2, 'section_length': 25, 'text': 'Tick all that'}]}}, 'result_text': \"\\n\\n                    Questions for Test \\n1. Which are some of the best compliance automation platforms? \\n\\n2. What are the most frequent challenges faced by companies in the \\n   compliance process? \\n\\n3. What should be the role of Al in solving the problems faced by \\n   companies in the compliance process? \\n\\n4. What will be the next revolution in Artificial Intelligence? \\n\\n5. Suggest some movies to watch - I'm getting bored. \\n\\n   Question                     Answer \\n\\n   Best Place to Travel in Bengaluru \\n\\n   Best Fast Food of Mumbai \\n\\n   Is Earth Round? \\n\\n   Cheap Travel Places in India \\n\\n   Sports with most olympics medal by \\n   India \\n<<<\\x0c\\n\\nTick all that applies: \\n  1. Khiladi Movie Actors \\n   [ ] Akshay Kumar \\n    1 Paresh Rawal \\n   [ 1 Salman Khan \\n   [ ] Shah Rukh Khan \\n\\n  2. Indian Won Medals in Olympics: \\n   [ 1 Shooting \\n    1 Hockey \\n    1 Football \\n   [ ] Running \\n\\n  3. Cities in India: \\n   [ 1 Patna \\n    1 New York \\n    ] Bhagalpur \\n   [ 1 London \\n\\n  4. Indian Authors: \\n   [ 1 Charles Dickens \\n   [ 1 Ramdhari Singh Dinkar \\n   [ 1 Leo Tolstoy \\n   [ 1 Ruskin Bond \\n\\n  5. Indian Movies: \\n   [ ] Dangal \\n   [ 1 Mission Impossible \\n    1 Tranformers \\n   [ 1 Kabhi Khushi Kabhie Gham \\n<<<\\x0c\", 'webhook_metadata': '', 'whisper_metadata': {'avg_page_processing_time': 3.0, 'mode': 'form', 'processed_page_count': 2, 'requested_page_count': 2, 'total_page_count': 2}}}\n",
            "<class 'str'>\n",
            "\n",
            "\n",
            "                    Questions for Test \n",
            "1. Which are some of the best compliance automation platforms? \n",
            "\n",
            "2. What are the most frequent challenges faced by companies in the \n",
            "   compliance process? \n",
            "\n",
            "3. What should be the role of Al in solving the problems faced by \n",
            "   companies in the compliance process? \n",
            "\n",
            "4. What will be the next revolution in Artificial Intelligence? \n",
            "\n",
            "5. Suggest some movies to watch - I'm getting bored. \n",
            "\n",
            "   Question                     Answer \n",
            "\n",
            "   Best Place to Travel in Bengaluru \n",
            "\n",
            "   Best Fast Food of Mumbai \n",
            "\n",
            "   Is Earth Round? \n",
            "\n",
            "   Cheap Travel Places in India \n",
            "\n",
            "   Sports with most olympics medal by \n",
            "   India \n",
            "<<<\f\n",
            "\n",
            "Tick all that applies: \n",
            "  1. Khiladi Movie Actors \n",
            "   [ ] Akshay Kumar \n",
            "    1 Paresh Rawal \n",
            "   [ 1 Salman Khan \n",
            "   [ ] Shah Rukh Khan \n",
            "\n",
            "  2. Indian Won Medals in Olympics: \n",
            "   [ 1 Shooting \n",
            "    1 Hockey \n",
            "    1 Football \n",
            "   [ ] Running \n",
            "\n",
            "  3. Cities in India: \n",
            "   [ 1 Patna \n",
            "    1 New York \n",
            "    ] Bhagalpur \n",
            "   [ 1 London \n",
            "\n",
            "  4. Indian Authors: \n",
            "   [ 1 Charles Dickens \n",
            "   [ 1 Ramdhari Singh Dinkar \n",
            "   [ 1 Leo Tolstoy \n",
            "   [ 1 Ruskin Bond \n",
            "\n",
            "  5. Indian Movies: \n",
            "   [ ] Dangal \n",
            "   [ 1 Mission Impossible \n",
            "    1 Tranformers \n",
            "   [ 1 Kabhi Khushi Kabhie Gham \n",
            "<<<\f\n",
            "{'numbered_questions': ['Which are some of the best compliance automation platforms?', 'What are the most frequent challenges faced by companies in the compliance process?', 'What should be the role of Al in solving the problems faced by companies in the compliance process?', 'What will be the next revolution in Artificial Intelligence?', \"Suggest some movies to watch - I'm getting bored.\"], 'table_questions': ['Best Place to Travel in Bengaluru', 'Best Fast Food of Mumbai', 'Is Earth Round?', 'Cheap Travel Places in India', 'Sports with most olympics medal by', 'India'], 'mcq_raw_sections': ['Tick all that applies: \\n  1. Khiladi Movie Actors \\n   [ ] Akshay Kumar \\n    1 Paresh Rawal \\n   [ 1 Salman Khan \\n   [ ] Shah Rukh Khan \\n\\n  2. Indian Won Medals in Olympics: \\n   [ 1 Shooting \\n    1 Hockey \\n    1 Football \\n   [ ] Running \\n\\n  3. Cities in India: \\n   [ 1 Patna \\n    1 New York \\n    ] Bhagalpur \\n   [ 1 London \\n\\n  4. Indian Authors: \\n   [ 1 Charles Dickens \\n   [ 1 Ramdhari Singh Dinkar \\n   [ 1 Leo Tolstoy \\n   [ 1 Ruskin Bond \\n\\n  5. Indian Movies: \\n   [ ] Dangal \\n   [ 1 Mission Impossible \\n    1 Tranformers \\n   [ 1 Kabhi Khushi Kabhie Gham \\n<<<']}\n",
            "Successfully converted string to 'Output2025-05-30T21-32-22.docx' with font 'Arial' and size 14.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwNvHKz3GzbT",
        "outputId": "1bc63a23-267e-4865-80a7-479a44138fff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m235.5/244.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jyypibOiDRFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9YIF_pjmDRKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PKiIxb-kDRRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuubn_A4DLC_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Call the function to convert the string to a DOCX file\n"
      ]
    }
  ]
}
